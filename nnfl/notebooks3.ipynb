{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#importing the required libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from keras.layers import Activation, Dense\n",
        "from tensorflow.keras.models import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "mLiWuAXIKbZo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vGIfvGppIPn2"
      },
      "outputs": [],
      "source": [
        "stock_data= pd.read_csv('Nifty50.csv' ,index_col='Date ') # Load the data from 'Nifty50.csv' into a pandas DataFrame.\n",
        "stock_data.head(3)  # Display the first 3 rows of the DataFrame\n",
        "stock_data.dropna(inplace=True) # Remove any rows in the DataFrame that contain missing values (NaN)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data.index = pd.to_datetime(stock_data.index) # Convert the index of the DataFrame to datetime format\n",
        "# Create a new figure with a specific size\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d')) # Set the date format of x-axis to 'YYYY-MM-DD'.\n",
        "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=60)) # Set the x-axis major ticks to occur every 60 days.\n",
        "x_dates = stock_data.index.date # Get the dates from the index of the DataFrame.\n",
        "plt.plot(x_dates, stock_data['High'], label='High') # Plot the 'High' column values against the dates.\n",
        "plt.plot(x_dates, stock_data['Low'], label='Low')   # Plot the 'Low' column values against the dates.\n",
        "plt.xlabel('Time Scale') # Label the x-axis as 'Time Scale'.\n",
        "plt.ylabel('scaled USD') # Label the y-axis as 'scaled USD'.\n",
        "plt.legend()             # Add a legend to the plot.\n",
        "plt.gcf().autofmt_xdate()# Auto-format the x-axis labels to fit into the figure area nicely.\n",
        "plt.show()               # Display the plot."
      ],
      "metadata": {
        "id": "nRJNvDmqIiVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data_last_2_months = stock_data[-60:] # Select the last 60 rows from the DataFrame\n",
        "# Create a new figure with a specific size\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d')) # Set the date format of x-axis to 'YYYY-MM-DD'.\n",
        "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=10))     # Set the x-axis major ticks to occur every 10 days.\n",
        "x_dates = stock_data_last_2_months.index.date  # Get the dates from the index of the DataFrame.\n",
        "plt.plot(x_dates, stock_data_last_2_months['High'], label='High') # Plot the 'High' column values against the dates.\n",
        "plt.plot(x_dates, stock_data_last_2_months['Low'], label='Low')   # Plot the 'Low' column values against the dates.\n",
        "plt.xlabel('Time Scale')  # Label the x-axis as 'Time Scale'.\n",
        "plt.ylabel('scaled USD')  # Label the y-axis as 'scaled USD'.\n",
        "plt.legend()              # Add a legend to the plot.\n",
        "plt.gcf().autofmt_xdate() # Auto-format the x-axis labels to fit into the figure area nicely.\n",
        "plt.show()  # Display the plot."
      ],
      "metadata": {
        "id": "Rpx1Ni4foK8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_y=stock_data['Close'] # 'target_y' is set as the 'Close' column of the DataFrame\n",
        "X_feat=stock_data.iloc[:,0:3] # 'X_feat' is set as the first 3 columns of the DataFrame"
      ],
      "metadata": {
        "id": "EeisUy2tIiX_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = StandardScaler() # Instantiate the StandardScaler. This will standardize features by removing the mean and scaling to unit variance.\n",
        "X_ft=sc.fit_transform(X_feat.values) # Fit the StandardScaler to the feature data (X_feat.values) and transform it.\n",
        "X_ft=pd.DataFrame(columns=X_feat.columns,data=X_ft,index=X_feat.index) # Convert the transformed data back into a DataFrame."
      ],
      "metadata": {
        "id": "y4dbBKosIia9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_split(data,n_steps):  # Define a function 'lstm_split' that takes in two arguments:\n",
        "  X,y=[],[]   # Initialize two empty lists, 'X' and 'y'.\n",
        "  for i in range(len(data)-n_steps+1):  # Loop over the data. The range is from 0 to the length of the data minus 'n_steps' plus 1.\n",
        "    X.append(data[i:i + n_steps,:-1])   # For each iteration, append a slice of the data (from 'i' to 'i + n_steps') excluding the last column to 'X'.\n",
        "    y. append(data[i + n_steps-1,-1])   # Append the last column of the data at the index 'i + n_steps - 1' to 'y'.\n",
        "  return np.array(X), np.array(y)       # Convert 'X' and 'y' to numpy arrays and return them."
      ],
      "metadata": {
        "id": "WLppM-k3Iid2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1,y1=lstm_split(stock_data.values, n_steps=2)\n",
        "# Use the 'lstm_split' function to split the stock data into sequences of length 2. 'X1' contains the sequences and 'y1' contains the corresponding labels.\n",
        "train_split=0.8  # Set the proportion of data to be used for training.\n",
        "split_idx=int(np.ceil(len(X1)*train_split))  # Calculate the index at which to split the data into training and testing sets.\n",
        "date_index=stock_data.index   # Get the dates from the index of the DataFrame.\n",
        "X_train,X_test=X1[:split_idx],X1[split_idx:]  # Split 'X1' into training and testing sets based on 'split_idx'.\n",
        "y_train,y_test=y1[:split_idx],y1[split_idx:]  # Split 'y1' into training and testing sets based on 'split_idx'.\n",
        "X_train_date,X_test_date=date_index[split_idx:],date_index[split_idx:]  # Split 'date_index' into training and testing sets based on 'split_idx'.\n",
        "print(X1.shape,X_train.shape,X_test.shape,y_test.shape) # Print the shapes of 'X1', 'X_train', 'X_test', and 'y_test' to verify the splits."
      ],
      "metadata": {
        "id": "Pk4RPwTAIigy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ff78df-c872-4b3f-e7a7-69747d77c81d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2052, 2, 5) (1642, 2, 5) (410, 2, 5) (410,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = Sequential()   # Initialize a Sequential model.\n",
        "lstm.add(LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "              activation='linear',return_sequences=True)) # Add an LSTM layer to the model. This layer has 32 units, uses a linear activation function, and returns sequences.\n",
        "# The input shape is set to match the shape of the training data.\n",
        "lstm.add(Dense(40,activation='linear')) # Add a Dense layer with 40 units and a linear activation function.\n",
        "lstm.add(Dense(80,activation='linear')) # Add a Dense layer with 80 units and a linear activation function.\n",
        "lstm.add(Dense(50,activation='linear')) # Add a Dense layer with 50 units and a linear activation function.\n",
        "lstm.add(Dense(20,activation='linear')) # Add a Dense layer with 20 units and a linear activation function.\n",
        "lstm.add(Dense(80,activation='linear')) # Add a Dense layer with 80 units and a linear activation function.\n",
        "lstm.add(Dense(10,activation='linear')) # Add a Dense layer with 10 units and a linear activation function.\n",
        "lstm.add(Dense(1))      # Add a final Dense layer with 1 unit. This will output the prediction of the model.\n",
        "lstm.compile(loss='mean_absolute_error',optimizer='adam') # Compile the model with the 'adam' optimizer and mean absolute error as the loss function.\n",
        "lstm.summary()    # Print a summary of the model. This will show the structure of the model and the number of parameters."
      ],
      "metadata": {
        "id": "8t_o1rgIIikG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=lstm.fit(X_train,y_train,epochs=300,batch_size=10,verbose=2,shuffle=False)\n",
        "# Fit the model to the training data.\n",
        "# 'epochs' is the number of times the learning algorithm will work through the entire training dataset.\n",
        "# 'batch_size' is the number of samples to work through before updating the internal model parameters.\n",
        "# 'verbose' is for turning on detailed logging during training (0 = silent, 1 = progress bar, 2 = one line per epoch).\n",
        "# 'shuffle' determines whether to shuffle the training data before each epoch. Here it is set to False."
      ],
      "metadata": {
        "id": "ZjC31srQRsSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d09b4a-e4da-4add-93c0-d2007af000e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "165/165 - 1s - loss: 4755.5249 - 593ms/epoch - 4ms/step\n",
            "Epoch 282/300\n",
            "165/165 - 1s - loss: 4704.5200 - 593ms/epoch - 4ms/step\n",
            "Epoch 283/300\n",
            "165/165 - 1s - loss: 4734.5093 - 611ms/epoch - 4ms/step\n",
            "Epoch 284/300\n",
            "165/165 - 1s - loss: 4824.4214 - 592ms/epoch - 4ms/step\n",
            "Epoch 285/300\n",
            "165/165 - 1s - loss: 4815.5698 - 585ms/epoch - 4ms/step\n",
            "Epoch 286/300\n",
            "165/165 - 1s - loss: 4831.0366 - 629ms/epoch - 4ms/step\n",
            "Epoch 287/300\n",
            "165/165 - 1s - loss: 4766.6523 - 606ms/epoch - 4ms/step\n",
            "Epoch 288/300\n",
            "165/165 - 1s - loss: 4815.0117 - 633ms/epoch - 4ms/step\n",
            "Epoch 289/300\n",
            "165/165 - 1s - loss: 4843.5537 - 617ms/epoch - 4ms/step\n",
            "Epoch 290/300\n",
            "165/165 - 1s - loss: 4767.4395 - 616ms/epoch - 4ms/step\n",
            "Epoch 291/300\n",
            "165/165 - 1s - loss: 4857.9321 - 606ms/epoch - 4ms/step\n",
            "Epoch 292/300\n",
            "165/165 - 1s - loss: 4814.3882 - 641ms/epoch - 4ms/step\n",
            "Epoch 293/300\n",
            "165/165 - 1s - loss: 4736.7139 - 651ms/epoch - 4ms/step\n",
            "Epoch 294/300\n",
            "165/165 - 1s - loss: 4816.4043 - 647ms/epoch - 4ms/step\n",
            "Epoch 295/300\n",
            "165/165 - 1s - loss: 4831.6616 - 689ms/epoch - 4ms/step\n",
            "Epoch 296/300\n",
            "165/165 - 1s - loss: 4812.8022 - 655ms/epoch - 4ms/step\n",
            "Epoch 297/300\n",
            "165/165 - 1s - loss: 4892.3345 - 939ms/epoch - 6ms/step\n",
            "Epoch 298/300\n",
            "165/165 - 1s - loss: 4753.1968 - 926ms/epoch - 6ms/step\n",
            "Epoch 299/300\n",
            "165/165 - 1s - loss: 4763.8169 - 899ms/epoch - 5ms/step\n",
            "Epoch 300/300\n",
            "165/165 - 1s - loss: 4796.1768 - 636ms/epoch - 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss']) # Plot the loss values stored in the 'history' object. 'history.history['loss']' contains the loss values at the end of each epoch.\n",
        "plt.title('Model Loss')   # Set the title of the plot as 'Model Loss'.\n",
        "plt.ylabel('Loss')        # Label the y-axis as 'Loss'.\n",
        "plt.xlabel('Epoch')       # Label the x-axis as 'Epoch'.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7qOWrXptSmiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=lstm.predict(X_test)\n",
        "# Use the trained LSTM model to predict the target variable for the test data.\n",
        "# The 'predict' function returns the predicted values as a numpy array."
      ],
      "metadata": {
        "id": "XxXGrBgZSCxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_r = y_pred[:, 0, 0]  # 'y_pred_r' is set as the first column of the first dimension of 'y_pred'. This reshapes the prediction array for plotting.\n",
        "plt.figure(figsize=(15, 10))  # Create a new figure with a specific size (15 units wide by 10 units tall).\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d')) # Set the date format for the x-axis to 'YYYY-MM-DD'.\n",
        "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=60))     # Set the x-axis major ticks to occur every 60 days.\n",
        "x_dates = X_test_date   # Get the dates from the index of the DataFrame.\n",
        "plt.plot(x_dates[:410], y_pred_r[:410], label='Predicted')  # Plot the predicted values against the dates.\n",
        "plt.plot(x_dates[:410], y_test[:410], label='Real')         # Plot the real values against the dates.\n",
        "plt.xlabel('Time Scale')  # Label the x-axis as 'Time Scale'.\n",
        "plt.ylabel('USD')         # Label the y-axis as 'USD'.\n",
        "plt.legend()              # Add a legend to the plot.\n",
        "plt.gcf().autofmt_xdate() # Auto-format the x-axis labels to fit into the figure area nicely.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QdtZZX0HSCVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae = mean_absolute_error(y_test, y_pred_r)   # Calculate the Mean Absolute Error (MAE)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred_r)   # Calculate the Mean Absolute Percentage Error (MAPE) between the actual and predicted values.\n",
        "mse = mean_squared_error(y_test, y_pred_r)    # Calculate the Mean Squared Error (MSE) between the actual and predicted values.\n",
        "rmse = np.sqrt(mse)   # Calculate the Root Mean Squared Error (RMSE)\n",
        "# Print the calculated error metrics.\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)"
      ],
      "metadata": {
        "id": "2-kKgIY00H3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy of the model. This is done by subtracting the Mean Absolute Percentage Error (MAPE) from 1 and multiplying by 100.\n",
        "# This gives the percentage of predictions that fall within the acceptable error range.\n",
        "accurate = (1 - mape) * 100\n",
        "print(\"Model accurate:\", accurate)"
      ],
      "metadata": {
        "id": "jx7r5h_B0Hzz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}